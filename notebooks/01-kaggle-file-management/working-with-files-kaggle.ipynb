{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working With Directories and Files in Kaggle\n",
    "\n",
    "![Kaggle Banner](https://www.kaggle.com/static/images/site-logo.png)\n",
    "\n",
    "## Project Information\n",
    "- **Author**: Dr. Saad Laouadi\n",
    "- **Date**: April 16, 2025\n",
    "- **Dataset**: Heart Failure Synthetic Dataset\n",
    "- **Version**: 1.0\n",
    "- **Repository**: [GitHub: Kaggle-File-Management](https://github.com/dr-saad-la/kaggle-projects-kaggle-file-management) - Contains all code, additional analysis, and documentation for this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "This notebook demonstrates efficient techniques for file and directory management within the Kaggle environment. Whether you're participating in competitions, creating datasets, or sharing analysis, understanding how to properly navigate and manipulate the file system is essential for productive data science workflows.\n",
    "\n",
    "## Objectives\n",
    "- Explore Kaggle's file system structure and conventions\n",
    "- Implement robust file handling using pathlib and other modern Python libraries\n",
    "- Create reproducible data loading patterns for machine learning projects\n",
    "- Establish best practices for organizing complex data science projects\n",
    "\n",
    "## Technologies & Libraries\n",
    "```python\n",
    "# Core libraries\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Optional - for potential modeling demonstrations\n",
    "from sklearn.model_selection import train_test_split\n",
    "```\n",
    "\n",
    "## Dataset Description\n",
    "The Heart Failure Synthetic Dataset contains simulated medical records focused on heart failure conditions. We'll use this dataset to demonstrate file operations while also performing basic exploratory data analysis.\n",
    "\n",
    "## Key Features of This Notebook\n",
    "- **Path Management**: Using modern `pathlib` for cross-platform compatibility\n",
    "- **Error Handling**: Implementing robust error checking for file operations\n",
    "- **Workflow Optimization**: Techniques for efficient data loading and processing\n",
    "- **Project Organization**: Best practices for structuring machine learning projects\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Matters\n",
    "Proper file management is the foundation of reproducible data science. By establishing good practices early in your workflow, you can:\n",
    "\n",
    "- Create more maintainable code\n",
    "- Improve collaboration with team members\n",
    "- Ensure portability across different environments\n",
    "- Reduce errors in data processing pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Access\n",
    "\n",
    "This notebook uses the Heart Failure Synthetic Dataset available on Kaggle.\n",
    "\n",
    "**Note:** For detailed instructions on downloading this dataset using the Kaggle API for local use, please see the [GitHub repository README](https://github.com/dr-saad-la/kaggle-projects)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by exploring the Kaggle file system structure and implementing some best practices for working with datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T10:32:32.831403Z",
     "iopub.status.busy": "2025-04-19T10:32:32.830163Z",
     "iopub.status.idle": "2025-04-19T10:32:36.658914Z",
     "shell.execute_reply": "2025-04-19T10:32:36.657550Z",
     "shell.execute_reply.started": "2025-04-19T10:32:32.831357Z"
    }
   },
   "outputs": [],
   "source": [
    "# This will install watermark notebook extension to show \n",
    "# information about the working environment\n",
    "import sys\n",
    "!{sys.executable} -m pip install -q watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T10:32:36.662269Z",
     "iopub.status.busy": "2025-04-19T10:32:36.661848Z",
     "iopub.status.idle": "2025-04-19T10:32:36.688286Z",
     "shell.execute_reply": "2025-04-19T10:32:36.687343Z",
     "shell.execute_reply.started": "2025-04-19T10:32:36.662227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Showing Environment Information---------\n",
      "Author: Dr. Saad Laouadi\n",
      "\n",
      "sys    : 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
      "pathlib: 1.0.1\n",
      "polars : 1.9.0\n",
      "numpy  : 1.26.4\n",
      "pandas : 2.2.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import pandas as pd  \n",
    "import numpy as np   \n",
    "\n",
    "%reload_ext watermark\n",
    "\n",
    "print(\"--------- Showing Environment Information---------\")\n",
    "%watermark -a \"Dr. Saad Laouadi\"\n",
    "%watermark -iv    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Kaggle's File Structure\n",
    "Let's first explore the current working directory and its parent to understand Kaggle's notebook environment structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T10:32:36.689568Z",
     "iopub.status.busy": "2025-04-19T10:32:36.689324Z",
     "iopub.status.idle": "2025-04-19T10:32:36.698864Z",
     "shell.execute_reply": "2025-04-19T10:32:36.697800Z",
     "shell.execute_reply.started": "2025-04-19T10:32:36.689546Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /kaggle/working\n",
      "Parent directory: /kaggle\n",
      "\n",
      "Files and directories in current working directory:\n",
      "  .virtual_documents (DIRECTORY)\n",
      "\n",
      "Files and directories in parent directory:\n",
      "  lib (DIRECTORY)\n",
      "  input (DIRECTORY)\n",
      "  working (DIRECTORY)\n"
     ]
    }
   ],
   "source": [
    "# Explore the working directory structure\n",
    "cwd = Path('.').resolve()\n",
    "parent_dir = Path('..').resolve()\n",
    "\n",
    "print(f\"Current working directory: {cwd}\")\n",
    "print(f\"Parent directory: {parent_dir}\")\n",
    "\n",
    "# List contents with more descriptive output\n",
    "print(\"\\nFiles and directories in current working directory:\")\n",
    "for item in cwd.iterdir():\n",
    "    item_type = \"FILE\" if item.is_file() else \"DIRECTORY\"\n",
    "    print(f\"  {item.name} ({item_type})\")\n",
    "\n",
    "print(\"\\nFiles and directories in parent directory:\")\n",
    "for item in parent_dir.iterdir():\n",
    "    item_type = \"FILE\" if item.is_file() else \"DIRECTORY\"\n",
    "    print(f\"  {item.name} ({item_type})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T09:24:34.796137Z",
     "iopub.status.busy": "2025-04-19T09:24:34.795809Z",
     "iopub.status.idle": "2025-04-19T09:24:34.803275Z",
     "shell.execute_reply": "2025-04-19T09:24:34.802184Z",
     "shell.execute_reply.started": "2025-04-19T09:24:34.796116Z"
    }
   },
   "source": [
    "## Accessing the Input Directory\n",
    "\n",
    "In Kaggle, datasets are mounted in the /kaggle/input directory. Let's explore this location to see what data is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T10:32:36.702139Z",
     "iopub.status.busy": "2025-04-19T10:32:36.701332Z",
     "iopub.status.idle": "2025-04-19T10:32:36.727346Z",
     "shell.execute_reply": "2025-04-19T10:32:36.726107Z",
     "shell.execute_reply.started": "2025-04-19T10:32:36.702107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 files and 1 directories in /kaggle/input\n",
      "\n",
      "Directories:\n",
      "  heart-failure-prediction-synthetic-dataset (1 items)\n",
      "    ├── heart_failure_prediction.csv (FILE)\n"
     ]
    }
   ],
   "source": [
    "# Explore the input directory where datasets are mounted\n",
    "input_dir = Path('/kaggle/input').resolve()\n",
    "\n",
    "# Get files and directories\n",
    "try:\n",
    "    # Separate files and directories for clarity\n",
    "    files = [item for item in input_dir.iterdir() if item.is_file()]\n",
    "    directories = [item for item in input_dir.iterdir() if item.is_dir()]\n",
    "    \n",
    "    # Print organized results\n",
    "    print(f\"Found {len(files)} files and {len(directories)} directories in {input_dir}\")\n",
    "    \n",
    "    if files:\n",
    "        print(\"\\nFiles:\")\n",
    "        for file in files:\n",
    "            print(f\"  {file.name} ({file.stat().st_size / 1024:.2f} KB)\")\n",
    "    \n",
    "    if directories:\n",
    "        print(\"\\nDirectories:\")\n",
    "        for directory in directories:\n",
    "            dir_contents = list(directory.iterdir())\n",
    "            print(f\"  {directory.name} ({len(dir_contents)} items)\")\n",
    "            \n",
    "            # Show first few items in each directory\n",
    "            for item in dir_contents[:3]:  # Show only first 3 items\n",
    "                item_type = \"FILE\" if item.is_file() else \"DIR\"\n",
    "                print(f\"    ├── {item.name} ({item_type})\")\n",
    "            if len(dir_contents) > 3:\n",
    "                print(f\"    └── ... and {len(dir_contents) - 3} more items\")\n",
    "                \n",
    "except PermissionError:\n",
    "    print(f\"Permission denied accessing {input_dir}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Directory not found: {input_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Helper Function for Dataset Exploration\n",
    "Let's create a reusable function to explore dataset directories more thoroughly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T10:32:36.728692Z",
     "iopub.status.busy": "2025-04-19T10:32:36.728386Z",
     "iopub.status.idle": "2025-04-19T10:32:36.749680Z",
     "shell.execute_reply": "2025-04-19T10:32:36.748665Z",
     "shell.execute_reply.started": "2025-04-19T10:32:36.728664Z"
    }
   },
   "outputs": [],
   "source": [
    "def explore_dataset(dataset_path, max_files=5):\n",
    "    \"\"\"\n",
    "    Explore a dataset directory and return structured information.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path (Path): Path to the dataset directory\n",
    "        max_files (int): Maximum number of files to display per directory\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing dataset information\n",
    "    \"\"\"\n",
    "    dataset_info = {\n",
    "        \"name\": dataset_path.name,\n",
    "        \"path\": str(dataset_path),\n",
    "        \"file_count\": 0,\n",
    "        \"dir_count\": 0,\n",
    "        \"size_bytes\": 0,\n",
    "        \"files\": [],\n",
    "        \"directories\": []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # List all items\n",
    "        all_items = list(dataset_path.iterdir())\n",
    "        \n",
    "        # Count files and directories\n",
    "        files = [f for f in all_items if f.is_file()]\n",
    "        dirs = [d for d in all_items if d.is_dir()]\n",
    "        \n",
    "        dataset_info[\"file_count\"] = len(files)\n",
    "        dataset_info[\"dir_count\"] = len(dirs)\n",
    "        \n",
    "        # Calculate total size\n",
    "        for file in files:\n",
    "            size = file.stat().st_size\n",
    "            dataset_info[\"size_bytes\"] += size\n",
    "            \n",
    "            # Get file information\n",
    "            if len(dataset_info[\"files\"]) < max_files:\n",
    "                dataset_info[\"files\"].append({\n",
    "                    \"name\": file.name,\n",
    "                    \"extension\": file.suffix,\n",
    "                    \"size_bytes\": size,\n",
    "                    \"size_formatted\": f\"{size/1024/1024:.2f} MB\" if size > 1024*1024 else f\"{size/1024:.2f} KB\"\n",
    "                })\n",
    "        \n",
    "        # Get directory information\n",
    "        for directory in dirs:\n",
    "            dir_files = list(directory.glob('**/*'))\n",
    "            dir_info = {\n",
    "                \"name\": directory.name,\n",
    "                \"file_count\": len([f for f in dir_files if f.is_file()]),\n",
    "                \"sample_files\": [f.name for f in dir_files if f.is_file()][:3]\n",
    "            }\n",
    "            dataset_info[\"directories\"].append(dir_info)\n",
    "            \n",
    "        return dataset_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error exploring {dataset_path}: {e}\")\n",
    "        return dataset_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T10:32:36.751095Z",
     "iopub.status.busy": "2025-04-19T10:32:36.750772Z",
     "iopub.status.idle": "2025-04-19T10:32:36.766382Z",
     "shell.execute_reply": "2025-04-19T10:32:36.765390Z",
     "shell.execute_reply.started": "2025-04-19T10:32:36.751066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "DATASET: heart-failure-prediction-synthetic-dataset\n",
      "==================================================\n",
      "Total files: 1\n",
      "Total directories: 0\n",
      "Total size: 1.09 MB\n",
      "\n",
      "Sample files:\n",
      "  • heart_failure_prediction.csv (1.09 MB)\n"
     ]
    }
   ],
   "source": [
    "# Check the available datasets\n",
    "for dataset_dir in directories:\n",
    "    dataset_info = explore_dataset(dataset_dir)\n",
    "    \n",
    "    # Print formatted information\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"DATASET: {dataset_info['name']}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total files: {dataset_info['file_count']}\")\n",
    "    print(f\"Total directories: {dataset_info['dir_count']}\")\n",
    "    print(f\"Total size: {dataset_info['size_bytes']/1024/1024:.2f} MB\")\n",
    "    \n",
    "    print(\"\\nSample files:\")\n",
    "    for file in dataset_info[\"files\"]:\n",
    "        print(f\"  • {file['name']} ({file['size_formatted']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "After exploring the Kaggle file system structure, we've discovered several key insights:\n",
    "\n",
    "* Kaggle's environment organizes files in a predictable structure with `/kaggle/input` containing all mounted datasets\n",
    "* Each competition or dataset gets its own subdirectory with a standardized naming convention\n",
    "* Understanding this structure allows for creating more robust paths in analysis code\n",
    "* File permissions are read-only for input directories, requiring use of the `/kaggle/working` directory for outputs\n",
    "* Large datasets are efficiently mounted with minimal overhead, enabling fast access even to large files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Example: Working with a Heart Failure Dataset\n",
    "\n",
    "Now let's apply our file management knowledge to a real-world example by loading and examining the heart failure dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T10:32:36.767661Z",
     "iopub.status.busy": "2025-04-19T10:32:36.767392Z",
     "iopub.status.idle": "2025-04-19T10:32:36.876775Z",
     "shell.execute_reply": "2025-04-19T10:32:36.875509Z",
     "shell.execute_reply.started": "2025-04-19T10:32:36.767640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CSV files in heart-failure-prediction-synthetic-dataset\n",
      "\n",
      "Dataset Overview:\n",
      "- Rows: 10000\n",
      "- Columns: 20\n",
      "\n",
      "Column names:\n",
      "  - Age\n",
      "  - Gender\n",
      "  - Chest_Pain_Type\n",
      "  - Resting_BP\n",
      "  - Cholesterol\n",
      "  - Fasting_Blood_Sugar\n",
      "  - Resting_ECG\n",
      "  - Max_Heart_Rate\n",
      "  - Exercise_Induced_Angina\n",
      "  - Oldpeak\n",
      "  - Slope\n",
      "  - Num_Major_Vessels\n",
      "  - Thalassemia\n",
      "  - Diabetes\n",
      "  - Smoking_History\n",
      "  - Alcohol_Consumption\n",
      "  - Physical_Activity_Level\n",
      "  - Family_History\n",
      "  - BMI\n",
      "  - Heart_Failure\n",
      "\n",
      "Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Chest_Pain_Type</th>\n",
       "      <th>Resting_BP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>Fasting_Blood_Sugar</th>\n",
       "      <th>Resting_ECG</th>\n",
       "      <th>Max_Heart_Rate</th>\n",
       "      <th>Exercise_Induced_Angina</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Num_Major_Vessels</th>\n",
       "      <th>Thalassemia</th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>Smoking_History</th>\n",
       "      <th>Alcohol_Consumption</th>\n",
       "      <th>Physical_Activity_Level</th>\n",
       "      <th>Family_History</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Heart_Failure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69</td>\n",
       "      <td>Male</td>\n",
       "      <td>Atypical</td>\n",
       "      <td>106</td>\n",
       "      <td>250</td>\n",
       "      <td>1</td>\n",
       "      <td>ST-T Wave Abnormality</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>Flat</td>\n",
       "      <td>2</td>\n",
       "      <td>Normal</td>\n",
       "      <td>1</td>\n",
       "      <td>Former</td>\n",
       "      <td>Heavy</td>\n",
       "      <td>Low</td>\n",
       "      <td>0</td>\n",
       "      <td>36.92</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>Male</td>\n",
       "      <td>Non-anginal</td>\n",
       "      <td>124</td>\n",
       "      <td>396</td>\n",
       "      <td>1</td>\n",
       "      <td>Left Ventricular Hypertrophy</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>Downsloping</td>\n",
       "      <td>2</td>\n",
       "      <td>Reversible Defect</td>\n",
       "      <td>1</td>\n",
       "      <td>Current</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Low</td>\n",
       "      <td>0</td>\n",
       "      <td>36.92</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89</td>\n",
       "      <td>Female</td>\n",
       "      <td>Non-anginal</td>\n",
       "      <td>164</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>Left Ventricular Hypertrophy</td>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>Upsloping</td>\n",
       "      <td>1</td>\n",
       "      <td>Fixed Defect</td>\n",
       "      <td>1</td>\n",
       "      <td>Former</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Low</td>\n",
       "      <td>0</td>\n",
       "      <td>36.92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>78</td>\n",
       "      <td>Female</td>\n",
       "      <td>Typical</td>\n",
       "      <td>116</td>\n",
       "      <td>297</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "      <td>Reversible Defect</td>\n",
       "      <td>1</td>\n",
       "      <td>Former</td>\n",
       "      <td>Heavy</td>\n",
       "      <td>Low</td>\n",
       "      <td>1</td>\n",
       "      <td>36.92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>Male</td>\n",
       "      <td>Non-anginal</td>\n",
       "      <td>88</td>\n",
       "      <td>386</td>\n",
       "      <td>1</td>\n",
       "      <td>ST-T Wave Abnormality</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>Upsloping</td>\n",
       "      <td>3</td>\n",
       "      <td>Fixed Defect</td>\n",
       "      <td>0</td>\n",
       "      <td>Never</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>Low</td>\n",
       "      <td>1</td>\n",
       "      <td>36.92</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Gender Chest_Pain_Type  Resting_BP  Cholesterol  Fasting_Blood_Sugar  \\\n",
       "0   69    Male        Atypical         106          250                    1   \n",
       "1   32    Male     Non-anginal         124          396                    1   \n",
       "2   89  Female     Non-anginal         164          256                    1   \n",
       "3   78  Female         Typical         116          297                    1   \n",
       "4   38    Male     Non-anginal          88          386                    1   \n",
       "\n",
       "                    Resting_ECG  Max_Heart_Rate  Exercise_Induced_Angina  \\\n",
       "0         ST-T Wave Abnormality             171                        0   \n",
       "1  Left Ventricular Hypertrophy              73                        0   \n",
       "2  Left Ventricular Hypertrophy             157                        0   \n",
       "3                        Normal             163                        1   \n",
       "4         ST-T Wave Abnormality             123                        1   \n",
       "\n",
       "   Oldpeak        Slope  Num_Major_Vessels        Thalassemia  Diabetes  \\\n",
       "0     0.92         Flat                  2             Normal         1   \n",
       "1     0.92  Downsloping                  2  Reversible Defect         1   \n",
       "2     0.92    Upsloping                  1       Fixed Defect         1   \n",
       "3     0.92         Flat                  1  Reversible Defect         1   \n",
       "4     0.92    Upsloping                  3       Fixed Defect         0   \n",
       "\n",
       "  Smoking_History Alcohol_Consumption Physical_Activity_Level  Family_History  \\\n",
       "0          Former               Heavy                     Low               0   \n",
       "1         Current                 NaN                     Low               0   \n",
       "2          Former                 NaN                     Low               0   \n",
       "3          Former               Heavy                     Low               1   \n",
       "4           Never            Moderate                     Low               1   \n",
       "\n",
       "     BMI  Heart_Failure  \n",
       "0  36.92              1  \n",
       "1  36.92              1  \n",
       "2  36.92              0  \n",
       "3  36.92              0  \n",
       "4  36.92              1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basic statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Resting_BP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>Fasting_Blood_Sugar</th>\n",
       "      <th>Max_Heart_Rate</th>\n",
       "      <th>Exercise_Induced_Angina</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>Num_Major_Vessels</th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>Family_History</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Heart_Failure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>1.000000e+04</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>1.000000e+04</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>58.584900</td>\n",
       "      <td>139.56920</td>\n",
       "      <td>247.206200</td>\n",
       "      <td>0.505400</td>\n",
       "      <td>129.346600</td>\n",
       "      <td>0.507200</td>\n",
       "      <td>9.200000e-01</td>\n",
       "      <td>1.481400</td>\n",
       "      <td>0.501200</td>\n",
       "      <td>0.506300</td>\n",
       "      <td>3.692000e+01</td>\n",
       "      <td>0.503600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>23.645835</td>\n",
       "      <td>34.86205</td>\n",
       "      <td>86.862739</td>\n",
       "      <td>0.499996</td>\n",
       "      <td>40.316689</td>\n",
       "      <td>0.499973</td>\n",
       "      <td>6.250868e-14</td>\n",
       "      <td>1.117488</td>\n",
       "      <td>0.500024</td>\n",
       "      <td>0.499985</td>\n",
       "      <td>7.176840e-13</td>\n",
       "      <td>0.500012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>80.00000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.200000e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.692000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>38.000000</td>\n",
       "      <td>109.00000</td>\n",
       "      <td>171.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.200000e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.692000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>59.000000</td>\n",
       "      <td>140.00000</td>\n",
       "      <td>247.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.200000e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.692000e+01</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>79.000000</td>\n",
       "      <td>170.00000</td>\n",
       "      <td>322.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.200000e-01</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.692000e+01</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>99.000000</td>\n",
       "      <td>199.00000</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.200000e-01</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.692000e+01</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Age   Resting_BP   Cholesterol  Fasting_Blood_Sugar  \\\n",
       "count  10000.000000  10000.00000  10000.000000         10000.000000   \n",
       "mean      58.584900    139.56920    247.206200             0.505400   \n",
       "std       23.645835     34.86205     86.862739             0.499996   \n",
       "min       18.000000     80.00000    100.000000             0.000000   \n",
       "25%       38.000000    109.00000    171.000000             0.000000   \n",
       "50%       59.000000    140.00000    247.000000             1.000000   \n",
       "75%       79.000000    170.00000    322.000000             1.000000   \n",
       "max       99.000000    199.00000    399.000000             1.000000   \n",
       "\n",
       "       Max_Heart_Rate  Exercise_Induced_Angina       Oldpeak  \\\n",
       "count    10000.000000             10000.000000  1.000000e+04   \n",
       "mean       129.346600                 0.507200  9.200000e-01   \n",
       "std         40.316689                 0.499973  6.250868e-14   \n",
       "min         60.000000                 0.000000  9.200000e-01   \n",
       "25%         95.000000                 0.000000  9.200000e-01   \n",
       "50%        130.000000                 1.000000  9.200000e-01   \n",
       "75%        164.000000                 1.000000  9.200000e-01   \n",
       "max        199.000000                 1.000000  9.200000e-01   \n",
       "\n",
       "       Num_Major_Vessels      Diabetes  Family_History           BMI  \\\n",
       "count       10000.000000  10000.000000    10000.000000  1.000000e+04   \n",
       "mean            1.481400      0.501200        0.506300  3.692000e+01   \n",
       "std             1.117488      0.500024        0.499985  7.176840e-13   \n",
       "min             0.000000      0.000000        0.000000  3.692000e+01   \n",
       "25%             0.000000      0.000000        0.000000  3.692000e+01   \n",
       "50%             1.000000      1.000000        1.000000  3.692000e+01   \n",
       "75%             2.000000      1.000000        1.000000  3.692000e+01   \n",
       "max             3.000000      1.000000        1.000000  3.692000e+01   \n",
       "\n",
       "       Heart_Failure  \n",
       "count   10000.000000  \n",
       "mean        0.503600  \n",
       "std         0.500012  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         1.000000  \n",
       "75%         1.000000  \n",
       "max         1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Locate our heart failure dataset\n",
    "heart_dataset_dir = [d for d in input_dir.iterdir() if \"heart-failure\" in d.name.lower()]\n",
    "\n",
    "if heart_dataset_dir:\n",
    "    # Get the directory\n",
    "    dataset_path = heart_dataset_dir[0]\n",
    "    \n",
    "    # Find CSV files in the dataset\n",
    "    csv_files = list(dataset_path.glob(\"**/*.csv\"))\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files in {dataset_path.name}\")\n",
    "    \n",
    "    if csv_files:\n",
    "        # Load the first CSV file\n",
    "        df = pd.read_csv(csv_files[0])\n",
    "        \n",
    "        # Display basic information\n",
    "        print(\"\\nDataset Overview:\")\n",
    "        print(f\"- Rows: {df.shape[0]}\")\n",
    "        print(f\"- Columns: {df.shape[1]}\")\n",
    "        print(\"\\nColumn names:\")\n",
    "        for col in df.columns:\n",
    "            print(f\"  - {col}\")\n",
    "            \n",
    "        # Display the first few rows\n",
    "        print(\"\\nSample data:\")\n",
    "        display(df.head())\n",
    "        \n",
    "        # Basic statistics\n",
    "        print(\"\\nBasic statistics:\")\n",
    "        display(df.describe())\n",
    "else:\n",
    "    print(\"Heart failure dataset not found. Please ensure the dataset is mounted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates how to:\n",
    "1. Dynamically locate a dataset without hardcoding paths\n",
    "2. Use globbing to find specific file types\n",
    "3. Create a robust data loading workflow that handles potential errors\n",
    "4. Perform basic data exploration after loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation Notes\n",
    "\n",
    "### Official Resources\n",
    "- [Kaggle API Documentation](https://github.com/Kaggle/kaggle-api)\n",
    "- [Kaggle Notebooks Documentation](https://www.kaggle.com/docs/notebooks)\n",
    "- [Kaggle Datasets Documentation](https://www.kaggle.com/docs/datasets)\n",
    "\n",
    "### Related Resources\n",
    "- [Python Pathlib Documentation](https://docs.python.org/3/library/pathlib.html)\n",
    "- [Best Practices for File Handling in Data Science](https://realpython.com/working-with-files-in-python/)\n",
    "- [Reproducible Data Science Guidelines](https://the-turing-way.netlify.app/reproducible-research/reproducible-research.html)\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "### Best Practices Learned\n",
    "1. **Use pathlib instead of os.path** for more readable and cross-platform compatible code\n",
    "2. **Implement error handling** when working with file operations to create robust workflows\n",
    "3. **Dynamic path discovery** reduces hardcoding and makes notebooks more portable\n",
    "4. **Structured file exploration** helps understand complex dataset organizations\n",
    "5. **Documenting file structures** improves reproducibility and collaboration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "- Apply these file management techniques to your own projects\n",
    "- Extend the helper functions to create a reusable file management toolkit\n",
    "- Explore creating a standardized project structure for data science work\n",
    "- Implement versioning for datasets to track changes over time\n",
    "- Consider creating a pipeline for automated dataset validation and cleaning\n",
    "\n",
    "This notebook provides a foundation for effective file management in Kaggle environments. By applying these techniques, you can create more robust, maintainable, and reproducible data science workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6850895,
     "sourceId": 11004903,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python:3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

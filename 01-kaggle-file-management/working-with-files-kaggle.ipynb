{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11004903,"sourceType":"datasetVersion","datasetId":6850895}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Working With Directories and Files in Kaggle\n\n![Kaggle Banner](https://www.kaggle.com/static/images/site-logo.png)\n\n## Project Information\n- **Author**: Dr. Saad Laouadi\n- **Date**: April 16, 2025\n- **Dataset**: Heart Failure Synthetic Dataset\n- **Version**: 1.0\n- **Repository**: [GitHub: Kaggle-File-Management](https://github.com/dr-saad-la/kaggle-projects-kaggle-file-management) - Contains all code, additional analysis, and documentation for this project","metadata":{}},{"cell_type":"markdown","source":"## Project Overview\nThis notebook demonstrates efficient techniques for file and directory management within the Kaggle environment. Whether you're participating in competitions, creating datasets, or sharing analysis, understanding how to properly navigate and manipulate the file system is essential for productive data science workflows.\n\n## Objectives\n- Explore Kaggle's file system structure and conventions\n- Implement robust file handling using pathlib and other modern Python libraries\n- Create reproducible data loading patterns for machine learning projects\n- Establish best practices for organizing complex data science projects\n\n## Technologies & Libraries\n```python\n# Core libraries\nfrom pathlib import Path\nimport os\nimport glob\n\n# Data processing\nimport pandas as pd\nimport numpy as np\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Optional - for potential modeling demonstrations\nfrom sklearn.model_selection import train_test_split\n```\n\n## Dataset Description\nThe Heart Failure Synthetic Dataset contains simulated medical records focused on heart failure conditions. We'll use this dataset to demonstrate file operations while also performing basic exploratory data analysis.\n\n## Key Features of This Notebook\n- **Path Management**: Using modern `pathlib` for cross-platform compatibility\n- **Error Handling**: Implementing robust error checking for file operations\n- **Workflow Optimization**: Techniques for efficient data loading and processing\n- **Project Organization**: Best practices for structuring machine learning projects\n\n---\n\n### Why This Matters\nProper file management is the foundation of reproducible data science. By establishing good practices early in your workflow, you can:\n\n- Create more maintainable code\n- Improve collaboration with team members\n- Ensure portability across different environments\n- Reduce errors in data processing pipelines","metadata":{}},{"cell_type":"markdown","source":"## Dataset Access\n\nThis notebook uses the Heart Failure Synthetic Dataset available on Kaggle.\n\n**Note:** For detailed instructions on downloading this dataset using the Kaggle API for local use, please see the [GitHub repository README](https://github.com/dr-saad-la/kaggle-projects).","metadata":{}},{"cell_type":"markdown","source":"Let's begin by exploring the Kaggle file system structure and implementing some best practices for working with datasets.","metadata":{}},{"cell_type":"code","source":"# This will install watermark notebook extension to show \n# information about the working environment\nimport sys\n!{sys.executable} -m pip install -q watermark","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T10:32:32.830163Z","iopub.execute_input":"2025-04-19T10:32:32.831403Z","iopub.status.idle":"2025-04-19T10:32:36.658914Z","shell.execute_reply.started":"2025-04-19T10:32:32.831357Z","shell.execute_reply":"2025-04-19T10:32:36.657550Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# Environment Setup\nfrom pathlib import Path\nimport os\nimport glob\n\nimport pandas as pd  \nimport numpy as np   \n\n%reload_ext watermark\n\nprint(\"--------- Showing Environment Information---------\")\n%watermark -a \"Dr. Saad Laouadi\"\n%watermark -iv    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T10:32:36.661848Z","iopub.execute_input":"2025-04-19T10:32:36.662269Z","iopub.status.idle":"2025-04-19T10:32:36.688286Z","shell.execute_reply.started":"2025-04-19T10:32:36.662227Z","shell.execute_reply":"2025-04-19T10:32:36.687343Z"}},"outputs":[{"name":"stdout","text":"--------- Showing Environment Information---------\nAuthor: Dr. Saad Laouadi\n\nsys    : 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\npathlib: 1.0.1\npolars : 1.9.0\nnumpy  : 1.26.4\npandas : 2.2.3\n\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"## Understanding Kaggle's File Structure\nLet's first explore the current working directory and its parent to understand Kaggle's notebook environment structure:","metadata":{}},{"cell_type":"code","source":"# Explore the working directory structure\ncwd = Path('.').resolve()\nparent_dir = Path('..').resolve()\n\nprint(f\"Current working directory: {cwd}\")\nprint(f\"Parent directory: {parent_dir}\")\n\n# List contents with more descriptive output\nprint(\"\\nFiles and directories in current working directory:\")\nfor item in cwd.iterdir():\n    item_type = \"FILE\" if item.is_file() else \"DIRECTORY\"\n    print(f\"  {item.name} ({item_type})\")\n\nprint(\"\\nFiles and directories in parent directory:\")\nfor item in parent_dir.iterdir():\n    item_type = \"FILE\" if item.is_file() else \"DIRECTORY\"\n    print(f\"  {item.name} ({item_type})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T10:32:36.689324Z","iopub.execute_input":"2025-04-19T10:32:36.689568Z","iopub.status.idle":"2025-04-19T10:32:36.698864Z","shell.execute_reply.started":"2025-04-19T10:32:36.689546Z","shell.execute_reply":"2025-04-19T10:32:36.697800Z"}},"outputs":[{"name":"stdout","text":"Current working directory: /kaggle/working\nParent directory: /kaggle\n\nFiles and directories in current working directory:\n  .virtual_documents (DIRECTORY)\n\nFiles and directories in parent directory:\n  lib (DIRECTORY)\n  input (DIRECTORY)\n  working (DIRECTORY)\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"## Accessing the Input Directory\n\nIn Kaggle, datasets are mounted in the /kaggle/input directory. Let's explore this location to see what data is available:","metadata":{"execution":{"iopub.status.busy":"2025-04-19T09:24:34.795809Z","iopub.execute_input":"2025-04-19T09:24:34.796137Z","iopub.status.idle":"2025-04-19T09:24:34.803275Z","shell.execute_reply.started":"2025-04-19T09:24:34.796116Z","shell.execute_reply":"2025-04-19T09:24:34.802184Z"}}},{"cell_type":"code","source":"# Explore the input directory where datasets are mounted\ninput_dir = Path('/kaggle/input').resolve()\n\n# Get files and directories\ntry:\n    # Separate files and directories for clarity\n    files = [item for item in input_dir.iterdir() if item.is_file()]\n    directories = [item for item in input_dir.iterdir() if item.is_dir()]\n    \n    # Print organized results\n    print(f\"Found {len(files)} files and {len(directories)} directories in {input_dir}\")\n    \n    if files:\n        print(\"\\nFiles:\")\n        for file in files:\n            print(f\"  {file.name} ({file.stat().st_size / 1024:.2f} KB)\")\n    \n    if directories:\n        print(\"\\nDirectories:\")\n        for directory in directories:\n            dir_contents = list(directory.iterdir())\n            print(f\"  {directory.name} ({len(dir_contents)} items)\")\n            \n            # Show first few items in each directory\n            for item in dir_contents[:3]:  # Show only first 3 items\n                item_type = \"FILE\" if item.is_file() else \"DIR\"\n                print(f\"    ├── {item.name} ({item_type})\")\n            if len(dir_contents) > 3:\n                print(f\"    └── ... and {len(dir_contents) - 3} more items\")\n                \nexcept PermissionError:\n    print(f\"Permission denied accessing {input_dir}\")\nexcept FileNotFoundError:\n    print(f\"Directory not found: {input_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T10:32:36.701332Z","iopub.execute_input":"2025-04-19T10:32:36.702139Z","iopub.status.idle":"2025-04-19T10:32:36.727346Z","shell.execute_reply.started":"2025-04-19T10:32:36.702107Z","shell.execute_reply":"2025-04-19T10:32:36.726107Z"}},"outputs":[{"name":"stdout","text":"Found 0 files and 1 directories in /kaggle/input\n\nDirectories:\n  heart-failure-prediction-synthetic-dataset (1 items)\n    ├── heart_failure_prediction.csv (FILE)\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"## Creating a Helper Function for Dataset Exploration\nLet's create a reusable function to explore dataset directories more thoroughly:","metadata":{}},{"cell_type":"code","source":"def explore_dataset(dataset_path, max_files=5):\n    \"\"\"\n    Explore a dataset directory and return structured information.\n    \n    Args:\n        dataset_path (Path): Path to the dataset directory\n        max_files (int): Maximum number of files to display per directory\n        \n    Returns:\n        dict: Dictionary containing dataset information\n    \"\"\"\n    dataset_info = {\n        \"name\": dataset_path.name,\n        \"path\": str(dataset_path),\n        \"file_count\": 0,\n        \"dir_count\": 0,\n        \"size_bytes\": 0,\n        \"files\": [],\n        \"directories\": []\n    }\n    \n    try:\n        # List all items\n        all_items = list(dataset_path.iterdir())\n        \n        # Count files and directories\n        files = [f for f in all_items if f.is_file()]\n        dirs = [d for d in all_items if d.is_dir()]\n        \n        dataset_info[\"file_count\"] = len(files)\n        dataset_info[\"dir_count\"] = len(dirs)\n        \n        # Calculate total size\n        for file in files:\n            size = file.stat().st_size\n            dataset_info[\"size_bytes\"] += size\n            \n            # Get file information\n            if len(dataset_info[\"files\"]) < max_files:\n                dataset_info[\"files\"].append({\n                    \"name\": file.name,\n                    \"extension\": file.suffix,\n                    \"size_bytes\": size,\n                    \"size_formatted\": f\"{size/1024/1024:.2f} MB\" if size > 1024*1024 else f\"{size/1024:.2f} KB\"\n                })\n        \n        # Get directory information\n        for directory in dirs:\n            dir_files = list(directory.glob('**/*'))\n            dir_info = {\n                \"name\": directory.name,\n                \"file_count\": len([f for f in dir_files if f.is_file()]),\n                \"sample_files\": [f.name for f in dir_files if f.is_file()][:3]\n            }\n            dataset_info[\"directories\"].append(dir_info)\n            \n        return dataset_info\n        \n    except Exception as e:\n        print(f\"Error exploring {dataset_path}: {e}\")\n        return dataset_info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T10:32:36.728386Z","iopub.execute_input":"2025-04-19T10:32:36.728692Z","iopub.status.idle":"2025-04-19T10:32:36.749680Z","shell.execute_reply.started":"2025-04-19T10:32:36.728664Z","shell.execute_reply":"2025-04-19T10:32:36.748665Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Check the available datasets\nfor dataset_dir in directories:\n    dataset_info = explore_dataset(dataset_dir)\n    \n    # Print formatted information\n    print(f\"\\n{'='*50}\")\n    print(f\"DATASET: {dataset_info['name']}\")\n    print(f\"{'='*50}\")\n    print(f\"Total files: {dataset_info['file_count']}\")\n    print(f\"Total directories: {dataset_info['dir_count']}\")\n    print(f\"Total size: {dataset_info['size_bytes']/1024/1024:.2f} MB\")\n    \n    print(\"\\nSample files:\")\n    for file in dataset_info[\"files\"]:\n        print(f\"  • {file['name']} ({file['size_formatted']})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T10:32:36.750772Z","iopub.execute_input":"2025-04-19T10:32:36.751095Z","iopub.status.idle":"2025-04-19T10:32:36.766382Z","shell.execute_reply.started":"2025-04-19T10:32:36.751066Z","shell.execute_reply":"2025-04-19T10:32:36.765390Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nDATASET: heart-failure-prediction-synthetic-dataset\n==================================================\nTotal files: 1\nTotal directories: 0\nTotal size: 1.09 MB\n\nSample files:\n  • heart_failure_prediction.csv (1.09 MB)\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"## Results Summary\n\nAfter exploring the Kaggle file system structure, we've discovered several key insights:\n\n* Kaggle's environment organizes files in a predictable structure with `/kaggle/input` containing all mounted datasets\n* Each competition or dataset gets its own subdirectory with a standardized naming convention\n* Understanding this structure allows for creating more robust paths in analysis code\n* File permissions are read-only for input directories, requiring use of the `/kaggle/working` directory for outputs\n* Large datasets are efficiently mounted with minimal overhead, enabling fast access even to large files","metadata":{}},{"cell_type":"markdown","source":"## Practical Example: Working with a Heart Failure Dataset\n\nNow let's apply our file management knowledge to a real-world example by loading and examining the heart failure dataset:","metadata":{}},{"cell_type":"code","source":"# Locate our heart failure dataset\nheart_dataset_dir = [d for d in input_dir.iterdir() if \"heart-failure\" in d.name.lower()]\n\nif heart_dataset_dir:\n    # Get the directory\n    dataset_path = heart_dataset_dir[0]\n    \n    # Find CSV files in the dataset\n    csv_files = list(dataset_path.glob(\"**/*.csv\"))\n    \n    print(f\"Found {len(csv_files)} CSV files in {dataset_path.name}\")\n    \n    if csv_files:\n        # Load the first CSV file\n        df = pd.read_csv(csv_files[0])\n        \n        # Display basic information\n        print(\"\\nDataset Overview:\")\n        print(f\"- Rows: {df.shape[0]}\")\n        print(f\"- Columns: {df.shape[1]}\")\n        print(\"\\nColumn names:\")\n        for col in df.columns:\n            print(f\"  - {col}\")\n            \n        # Display the first few rows\n        print(\"\\nSample data:\")\n        display(df.head())\n        \n        # Basic statistics\n        print(\"\\nBasic statistics:\")\n        display(df.describe())\nelse:\n    print(\"Heart failure dataset not found. Please ensure the dataset is mounted.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T10:32:36.767392Z","iopub.execute_input":"2025-04-19T10:32:36.767661Z","iopub.status.idle":"2025-04-19T10:32:36.876775Z","shell.execute_reply.started":"2025-04-19T10:32:36.767640Z","shell.execute_reply":"2025-04-19T10:32:36.875509Z"}},"outputs":[{"name":"stdout","text":"Found 1 CSV files in heart-failure-prediction-synthetic-dataset\n\nDataset Overview:\n- Rows: 10000\n- Columns: 20\n\nColumn names:\n  - Age\n  - Gender\n  - Chest_Pain_Type\n  - Resting_BP\n  - Cholesterol\n  - Fasting_Blood_Sugar\n  - Resting_ECG\n  - Max_Heart_Rate\n  - Exercise_Induced_Angina\n  - Oldpeak\n  - Slope\n  - Num_Major_Vessels\n  - Thalassemia\n  - Diabetes\n  - Smoking_History\n  - Alcohol_Consumption\n  - Physical_Activity_Level\n  - Family_History\n  - BMI\n  - Heart_Failure\n\nSample data:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   Age  Gender Chest_Pain_Type  Resting_BP  Cholesterol  Fasting_Blood_Sugar  \\\n0   69    Male        Atypical         106          250                    1   \n1   32    Male     Non-anginal         124          396                    1   \n2   89  Female     Non-anginal         164          256                    1   \n3   78  Female         Typical         116          297                    1   \n4   38    Male     Non-anginal          88          386                    1   \n\n                    Resting_ECG  Max_Heart_Rate  Exercise_Induced_Angina  \\\n0         ST-T Wave Abnormality             171                        0   \n1  Left Ventricular Hypertrophy              73                        0   \n2  Left Ventricular Hypertrophy             157                        0   \n3                        Normal             163                        1   \n4         ST-T Wave Abnormality             123                        1   \n\n   Oldpeak        Slope  Num_Major_Vessels        Thalassemia  Diabetes  \\\n0     0.92         Flat                  2             Normal         1   \n1     0.92  Downsloping                  2  Reversible Defect         1   \n2     0.92    Upsloping                  1       Fixed Defect         1   \n3     0.92         Flat                  1  Reversible Defect         1   \n4     0.92    Upsloping                  3       Fixed Defect         0   \n\n  Smoking_History Alcohol_Consumption Physical_Activity_Level  Family_History  \\\n0          Former               Heavy                     Low               0   \n1         Current                 NaN                     Low               0   \n2          Former                 NaN                     Low               0   \n3          Former               Heavy                     Low               1   \n4           Never            Moderate                     Low               1   \n\n     BMI  Heart_Failure  \n0  36.92              1  \n1  36.92              1  \n2  36.92              0  \n3  36.92              0  \n4  36.92              1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Gender</th>\n      <th>Chest_Pain_Type</th>\n      <th>Resting_BP</th>\n      <th>Cholesterol</th>\n      <th>Fasting_Blood_Sugar</th>\n      <th>Resting_ECG</th>\n      <th>Max_Heart_Rate</th>\n      <th>Exercise_Induced_Angina</th>\n      <th>Oldpeak</th>\n      <th>Slope</th>\n      <th>Num_Major_Vessels</th>\n      <th>Thalassemia</th>\n      <th>Diabetes</th>\n      <th>Smoking_History</th>\n      <th>Alcohol_Consumption</th>\n      <th>Physical_Activity_Level</th>\n      <th>Family_History</th>\n      <th>BMI</th>\n      <th>Heart_Failure</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>69</td>\n      <td>Male</td>\n      <td>Atypical</td>\n      <td>106</td>\n      <td>250</td>\n      <td>1</td>\n      <td>ST-T Wave Abnormality</td>\n      <td>171</td>\n      <td>0</td>\n      <td>0.92</td>\n      <td>Flat</td>\n      <td>2</td>\n      <td>Normal</td>\n      <td>1</td>\n      <td>Former</td>\n      <td>Heavy</td>\n      <td>Low</td>\n      <td>0</td>\n      <td>36.92</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>32</td>\n      <td>Male</td>\n      <td>Non-anginal</td>\n      <td>124</td>\n      <td>396</td>\n      <td>1</td>\n      <td>Left Ventricular Hypertrophy</td>\n      <td>73</td>\n      <td>0</td>\n      <td>0.92</td>\n      <td>Downsloping</td>\n      <td>2</td>\n      <td>Reversible Defect</td>\n      <td>1</td>\n      <td>Current</td>\n      <td>NaN</td>\n      <td>Low</td>\n      <td>0</td>\n      <td>36.92</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>89</td>\n      <td>Female</td>\n      <td>Non-anginal</td>\n      <td>164</td>\n      <td>256</td>\n      <td>1</td>\n      <td>Left Ventricular Hypertrophy</td>\n      <td>157</td>\n      <td>0</td>\n      <td>0.92</td>\n      <td>Upsloping</td>\n      <td>1</td>\n      <td>Fixed Defect</td>\n      <td>1</td>\n      <td>Former</td>\n      <td>NaN</td>\n      <td>Low</td>\n      <td>0</td>\n      <td>36.92</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>78</td>\n      <td>Female</td>\n      <td>Typical</td>\n      <td>116</td>\n      <td>297</td>\n      <td>1</td>\n      <td>Normal</td>\n      <td>163</td>\n      <td>1</td>\n      <td>0.92</td>\n      <td>Flat</td>\n      <td>1</td>\n      <td>Reversible Defect</td>\n      <td>1</td>\n      <td>Former</td>\n      <td>Heavy</td>\n      <td>Low</td>\n      <td>1</td>\n      <td>36.92</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>38</td>\n      <td>Male</td>\n      <td>Non-anginal</td>\n      <td>88</td>\n      <td>386</td>\n      <td>1</td>\n      <td>ST-T Wave Abnormality</td>\n      <td>123</td>\n      <td>1</td>\n      <td>0.92</td>\n      <td>Upsloping</td>\n      <td>3</td>\n      <td>Fixed Defect</td>\n      <td>0</td>\n      <td>Never</td>\n      <td>Moderate</td>\n      <td>Low</td>\n      <td>1</td>\n      <td>36.92</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\nBasic statistics:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                Age   Resting_BP   Cholesterol  Fasting_Blood_Sugar  \\\ncount  10000.000000  10000.00000  10000.000000         10000.000000   \nmean      58.584900    139.56920    247.206200             0.505400   \nstd       23.645835     34.86205     86.862739             0.499996   \nmin       18.000000     80.00000    100.000000             0.000000   \n25%       38.000000    109.00000    171.000000             0.000000   \n50%       59.000000    140.00000    247.000000             1.000000   \n75%       79.000000    170.00000    322.000000             1.000000   \nmax       99.000000    199.00000    399.000000             1.000000   \n\n       Max_Heart_Rate  Exercise_Induced_Angina       Oldpeak  \\\ncount    10000.000000             10000.000000  1.000000e+04   \nmean       129.346600                 0.507200  9.200000e-01   \nstd         40.316689                 0.499973  6.250868e-14   \nmin         60.000000                 0.000000  9.200000e-01   \n25%         95.000000                 0.000000  9.200000e-01   \n50%        130.000000                 1.000000  9.200000e-01   \n75%        164.000000                 1.000000  9.200000e-01   \nmax        199.000000                 1.000000  9.200000e-01   \n\n       Num_Major_Vessels      Diabetes  Family_History           BMI  \\\ncount       10000.000000  10000.000000    10000.000000  1.000000e+04   \nmean            1.481400      0.501200        0.506300  3.692000e+01   \nstd             1.117488      0.500024        0.499985  7.176840e-13   \nmin             0.000000      0.000000        0.000000  3.692000e+01   \n25%             0.000000      0.000000        0.000000  3.692000e+01   \n50%             1.000000      1.000000        1.000000  3.692000e+01   \n75%             2.000000      1.000000        1.000000  3.692000e+01   \nmax             3.000000      1.000000        1.000000  3.692000e+01   \n\n       Heart_Failure  \ncount   10000.000000  \nmean        0.503600  \nstd         0.500012  \nmin         0.000000  \n25%         0.000000  \n50%         1.000000  \n75%         1.000000  \nmax         1.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Resting_BP</th>\n      <th>Cholesterol</th>\n      <th>Fasting_Blood_Sugar</th>\n      <th>Max_Heart_Rate</th>\n      <th>Exercise_Induced_Angina</th>\n      <th>Oldpeak</th>\n      <th>Num_Major_Vessels</th>\n      <th>Diabetes</th>\n      <th>Family_History</th>\n      <th>BMI</th>\n      <th>Heart_Failure</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>10000.000000</td>\n      <td>10000.00000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>1.000000e+04</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>1.000000e+04</td>\n      <td>10000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>58.584900</td>\n      <td>139.56920</td>\n      <td>247.206200</td>\n      <td>0.505400</td>\n      <td>129.346600</td>\n      <td>0.507200</td>\n      <td>9.200000e-01</td>\n      <td>1.481400</td>\n      <td>0.501200</td>\n      <td>0.506300</td>\n      <td>3.692000e+01</td>\n      <td>0.503600</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>23.645835</td>\n      <td>34.86205</td>\n      <td>86.862739</td>\n      <td>0.499996</td>\n      <td>40.316689</td>\n      <td>0.499973</td>\n      <td>6.250868e-14</td>\n      <td>1.117488</td>\n      <td>0.500024</td>\n      <td>0.499985</td>\n      <td>7.176840e-13</td>\n      <td>0.500012</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>18.000000</td>\n      <td>80.00000</td>\n      <td>100.000000</td>\n      <td>0.000000</td>\n      <td>60.000000</td>\n      <td>0.000000</td>\n      <td>9.200000e-01</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>3.692000e+01</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>38.000000</td>\n      <td>109.00000</td>\n      <td>171.000000</td>\n      <td>0.000000</td>\n      <td>95.000000</td>\n      <td>0.000000</td>\n      <td>9.200000e-01</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>3.692000e+01</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>59.000000</td>\n      <td>140.00000</td>\n      <td>247.000000</td>\n      <td>1.000000</td>\n      <td>130.000000</td>\n      <td>1.000000</td>\n      <td>9.200000e-01</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>3.692000e+01</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>79.000000</td>\n      <td>170.00000</td>\n      <td>322.000000</td>\n      <td>1.000000</td>\n      <td>164.000000</td>\n      <td>1.000000</td>\n      <td>9.200000e-01</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>3.692000e+01</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>99.000000</td>\n      <td>199.00000</td>\n      <td>399.000000</td>\n      <td>1.000000</td>\n      <td>199.000000</td>\n      <td>1.000000</td>\n      <td>9.200000e-01</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>3.692000e+01</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":35},{"cell_type":"markdown","source":"This example demonstrates how to:\n1. Dynamically locate a dataset without hardcoding paths\n2. Use globbing to find specific file types\n3. Create a robust data loading workflow that handles potential errors\n4. Perform basic data exploration after loading","metadata":{}},{"cell_type":"markdown","source":"## Documentation Notes\n\n### Official Resources\n- [Kaggle API Documentation](https://github.com/Kaggle/kaggle-api)\n- [Kaggle Notebooks Documentation](https://www.kaggle.com/docs/notebooks)\n- [Kaggle Datasets Documentation](https://www.kaggle.com/docs/datasets)\n\n### Related Resources\n- [Python Pathlib Documentation](https://docs.python.org/3/library/pathlib.html)\n- [Best Practices for File Handling in Data Science](https://realpython.com/working-with-files-in-python/)\n- [Reproducible Data Science Guidelines](https://the-turing-way.netlify.app/reproducible-research/reproducible-research.html)\n\n## Conclusion\n\n### Best Practices Learned\n1. **Use pathlib instead of os.path** for more readable and cross-platform compatible code\n2. **Implement error handling** when working with file operations to create robust workflows\n3. **Dynamic path discovery** reduces hardcoding and makes notebooks more portable\n4. **Structured file exploration** helps understand complex dataset organizations\n5. **Documenting file structures** improves reproducibility and collaboration","metadata":{}},{"cell_type":"markdown","source":"### Next Steps\n- Apply these file management techniques to your own projects\n- Extend the helper functions to create a reusable file management toolkit\n- Explore creating a standardized project structure for data science work\n- Implement versioning for datasets to track changes over time\n- Consider creating a pipeline for automated dataset validation and cleaning\n\nThis notebook provides a foundation for effective file management in Kaggle environments. By applying these techniques, you can create more robust, maintainable, and reproducible data science workflows.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}